{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Judiciary Website to collect PFDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries & setting up the Notebook  \n",
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    " \n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def retries(record_url, tries=3):\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            soup = get_url(record_url)\n",
    "            return soup\n",
    "        except (ConnectionError, SSLError):\n",
    "            if i < tries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                return 'Con error'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper starts here - run Sun 14 May 2023 9.15 pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:33<00:00, 13.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4420 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_url = 'https://www.judiciary.uk/prevention-of-future-death-reports/page/{}/'\n",
    "# update the below number based on the number of 'pages' on the Judiciary website \n",
    "page_count = 442\n",
    "\n",
    "with requests.Session() as session:\n",
    "    record_urls = [ ]\n",
    "    for page in tqdm(range(1, page_count + 1)):\n",
    "        url = base_url.format(page)\n",
    "        try:\n",
    "            response = session.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            record_urls += [a['href'] for a in soup.find_all('a', {'class': 'card__link'})]\n",
    "        except (requests.exceptions.RequestException, ValueError, AttributeError) as e:\n",
    "            print(f\"Failed to process page {page}: {e}\")\n",
    "    print(f\"Collected {len(record_urls)} URLs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4420"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we check how many records/cases/urls were found on the Judiciary website\n",
    "len(record_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.judiciary.uk/prevention-of-future-death-reports/brenda-shaw-prevention-of-future-deaths-report/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.judiciary.uk/prevention-of-future-death-reports/jack-william-payton/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_urls[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the second loop that will go through the list of URLs from above to visit each individual record to pull out and store the text data into the a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_details(e_dict, record_count, record_url, details):\n",
    "    e_dict['index'] = record_count\n",
    "    e_dict['url'] = record_url\n",
    "    e_dict['reason'] = details\n",
    "    return e_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'select' occurr:  99%|█████████▉| 4383/4420 [29:42<00:15,  2.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/railwayrelateddeaths/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'select' occurr:  99%|█████████▉| 4384/4420 [29:43<00:14,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/service-personnel-deaths/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'select' occurr:  99%|█████████▉| 4385/4420 [29:43<00:13,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/product/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'select' occurr:  99%|█████████▉| 4386/4420 [29:43<00:12,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/policerelateddeaths/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'select' occurr:  99%|█████████▉| 4387/4420 [29:44<00:12,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/carehomehealth/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'select' occurr:  99%|█████████▉| 4388/4420 [29:44<00:12,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/statecustodydeath/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'select' occurr:  99%|█████████▉| 4390/4420 [29:45<00:10,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/hospitaldeath/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'select' occurr: 100%|██████████| 4420/4420 [29:58<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "reg_exp = re.compile(r\"’s\\s|s\\s|'s\\s\")\n",
    "text_cats = ['Date of report', 'Ref', 'Deceased name', 'Coroner name', 'Coroner Area', 'Category', \"This report is being sent to\"]\n",
    "#First, I create two lists, one for the PDFs and one for the text data\n",
    "record_text = []\n",
    "pdf_urls = []\n",
    "ref_list = []\n",
    "#I want to loop through each URL & pull out the death information and pdf link for downloading\n",
    "error_catching = []\n",
    "record_count = 0\n",
    "record_progress = tqdm(record_urls)\n",
    "\n",
    "for record_url in record_progress:\n",
    "    try:\n",
    "        error_dict = {}\n",
    "        #Calling the retries function\n",
    "        soup = retries(record_url, tries=5)\n",
    "        \n",
    "        if soup == 'Con error':\n",
    "            print(f\"{record_url} could not connect\")\n",
    "            error_catching.append(error_details(error_dict, record_count, record_url, 'Connection Error'))\n",
    "            record_count +=1\n",
    "            continue\n",
    "\n",
    "        #This gets all the text fields from the website to work with\n",
    "        death_info = soup.select_one('article.pfd_report_type-prevention-of-future-deaths').select('div > p')\n",
    "        \n",
    "        if not death_info:\n",
    "            print(f\"{record_url} produced no data\")\n",
    "            error_catching.append(error_details(error_dict, record_count, record_url, 'No Text Loaded'))\n",
    "            record_count +=1\n",
    "            continue\n",
    "            \n",
    "        #Our dictionary that will hold all of the text information that we will eventually append to \"record_text\"\n",
    "        blankdict = {}\n",
    "        \n",
    "        #This is to handle 1 annoying record with messed up html tags\n",
    "        if record_url == 'https://www.judiciary.uk/publications/roadsafety/':\n",
    "            strong = death_info[0].find_all('strong')\n",
    "            heads = ['date_of_report', 'ref', 'deceased_name', 'coroner_name', 'coroner_area', 'category']\n",
    "            for st, h in zip(strong,heads):\n",
    "                blankdict[h] = st.next_sibling.replace(':','').replace('Ref','').strip()\n",
    "        #And another record with wonky html\n",
    "        elif record_url == 'https://www.judiciary.uk/publications/helen-sheath/':\n",
    "            brs = death_info[0].text.split('\\n')\n",
    "            vals = []\n",
    "            for b in brs:\n",
    "                vals.append(b.split(':'))\n",
    "            for v in vals:\n",
    "                if v[0] == \"Coroners name\":\n",
    "                    alt = \"coroner_name\"\n",
    "                    blankdict[alt] = v[1].strip().replace('\\n','')\n",
    "                elif v[0] == \"Coroners Area\":\n",
    "                    alt = \"coroner_area\"\n",
    "                    blankdict[alt] = v[1].strip().replace('\\n','')\n",
    "                else:\n",
    "                    blankdict[v[0].strip().replace(' ','_').lower()] = v[1].strip().replace('\\n','')\n",
    "        else:        \n",
    "            #looping through all of the text categories for handling\n",
    "            for p in death_info:\n",
    "                #This checks for blank fields and if there is nothing, it skips it\n",
    "                if p.text.strip() == '':\n",
    "                    pass\n",
    "                #This checks for our \"Normal\" case in which a colon exists and the category is one of the ones we \n",
    "                #pre-specified above in the \"text_cats\" list\n",
    "                #We also need to account here for one strange record for \"Rebecca Evans\" which has a weird text error\n",
    "                #That we manually correct for\n",
    "                elif ':' in p.text and p.text.split(':')[0] in text_cats and not 'Rebecca-EvansR.pdf' in p.text:\n",
    "                    #Simply assigning the key and value from strings on either side of the colon, making everything \n",
    "                    #lower case and replacing spaces with underscores and also removing any stray semi-colons\n",
    "                    text_list = p.text.split(':')\n",
    "                    blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','').replace('\\xa0','')\n",
    "\n",
    "                elif 'Rebecca-EvansR.pdf' in p.text:\n",
    "                    #This deals with that singular odd record that currently exists as of 8 Nov 2019\n",
    "                    blankdict['category'] = p.text.split(':')[1].strip().replace('\\n','')\n",
    "                    \n",
    "                elif ':' not in p.text:\n",
    "                    #If the string doesn't have a colon, we can't split on it so have to get it into dictionary format\n",
    "                    #Using an alternate method that counts the length of the thing\n",
    "                    if any(x in p.text for x in text_cats):\n",
    "                        t = [x for x in text_cats if x in p.text][0]\n",
    "                        l = len(t)\n",
    "                        blankdict[t.replace(' ','_').lower()] = p.text[l+1:].replace('\\n','').replace('\\xa0','')\n",
    "                    elif 'Coroners Area' in p.text:\n",
    "                        blankdict['coroner_area'] = p.text[13:].strip().replace('\\n','').replace('\\xa0','')\n",
    "                    else:\n",
    "                        print(\"Something we haven't accounted for has happened\")\n",
    "\n",
    "                elif p.text.strip().count(\":\") == 2:\n",
    "                    #This corrects for one odd record in which there are 2 colons but should generalize to fix it for\n",
    "                    #any time this could happen, so long as it happens in the same way\n",
    "                    text_list = p.text.split(':')\n",
    "                    new_string = text_list[0] + text_list[1]\n",
    "                    new_name = re.sub(reg_exp, ' ', new_string).strip()\n",
    "                    blankdict[new_name.replace(' ','_').lower()] = text_list[2].strip().replace('\\n','').replace('\\xa0','')\n",
    "\n",
    "                elif ':' in p.text and p.text.split(':')[0] not in text_cats:\n",
    "                    #Some field names are in the form of \"name_of_decesased\" or \"name_of_coroner\" or are plural/\n",
    "                    #possessive so this smashes those into our preferred naming formats\n",
    "                    if 'Name of' in p.text:\n",
    "                        all_text = p.text.split(':')\n",
    "                        key_name = all_text[0].split(' ')\n",
    "                        blankdict[key_name[2].strip() + '_name'] = all_text[-1].strip()\n",
    "                    else:    \n",
    "                        new_name = re.sub(reg_exp, ' ', p.text)\n",
    "                        text_list = new_name.split(':')\n",
    "                        blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','').replace('\\xa0','')\n",
    "        blankdict['url'] = record_url\n",
    "        \n",
    "        #A small little check for duplicated ref names\n",
    "        try:\n",
    "            if not blankdict['ref']:\n",
    "                pass\n",
    "            elif blankdict['ref'] in ref_list:\n",
    "                blankdict['ref'] = blankdict['ref'] + 'A'\n",
    "            ref_list.append(blankdict['ref'])\n",
    "        except KeyError:\n",
    "            blankdict['ref'] = ''\n",
    "            \n",
    "        #This appends the final dict to the list\n",
    "        record_text.append(blankdict)\n",
    "        \n",
    "        #this is a seperate process to get the PDF URLs (no matter how many there are) and adds them to their own list   \n",
    "        urls = soup.select('li.related-content__item')\n",
    "        pdf_list = []\n",
    "        for url in urls:\n",
    "            pdf_list.append(url.findNext('a').get('href'))\n",
    "        pdf_urls.append(pdf_list)\n",
    "        \n",
    "        record_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        import sys\n",
    "        error_desc = f\"{str(e)} occurred for {record_url} when trying to work with {p}\"\n",
    "        error_n = f\"{record_count}: {str(e)}\"\n",
    "        short_desc = error_n[:50] + '...' if len(error_n) > 50 else error_n\n",
    "        record_progress.set_description(short_desc)\n",
    "        # print(error_desc)\n",
    "        error_catching.append(error_details(error_dict, record_count, record_url, error_desc))\n",
    "        \n",
    "        #Saving this in case we don't like the error catching.\n",
    "        #import sys\n",
    "        #raise type(e)(str(e) + '\\n' + 'Error for Record: {}, Field: {}'.format(record_url, p)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the third loop to save the PDFs using the deceased Ref as the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>url</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>305</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>462</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>462</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>462</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>462</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3964</th>\n",
       "      <td>462</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>'NoneType' object has no attribute 'select' oc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3965 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                url   \n",
       "0        27  https://www.judiciary.uk/prevention-of-future-...  \\\n",
       "1       180  https://www.judiciary.uk/prevention-of-future-...   \n",
       "2       192  https://www.judiciary.uk/prevention-of-future-...   \n",
       "3       305  https://www.judiciary.uk/prevention-of-future-...   \n",
       "4       384  https://www.judiciary.uk/prevention-of-future-...   \n",
       "...     ...                                                ...   \n",
       "3960    462  https://www.judiciary.uk/prevention-of-future-...   \n",
       "3961    462  https://www.judiciary.uk/prevention-of-future-...   \n",
       "3962    462  https://www.judiciary.uk/prevention-of-future-...   \n",
       "3963    462  https://www.judiciary.uk/prevention-of-future-...   \n",
       "3964    462  https://www.judiciary.uk/prevention-of-future-...   \n",
       "\n",
       "                                                 reason  \n",
       "0     'NoneType' object has no attribute 'select' oc...  \n",
       "1     'NoneType' object has no attribute 'select' oc...  \n",
       "2     'NoneType' object has no attribute 'select' oc...  \n",
       "3     'NoneType' object has no attribute 'select' oc...  \n",
       "4     'NoneType' object has no attribute 'select' oc...  \n",
       "...                                                 ...  \n",
       "3960  'NoneType' object has no attribute 'select' oc...  \n",
       "3961  'NoneType' object has no attribute 'select' oc...  \n",
       "3962  'NoneType' object has no attribute 'select' oc...  \n",
       "3963  'NoneType' object has no attribute 'select' oc...  \n",
       "3964  'NoneType' object has no attribute 'select' oc...  \n",
       "\n",
       "[3965 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Any errors should print out above, but you can also check the error_catching dict\n",
    "#Here we just turn it into a dataframe quickly to easily view\n",
    "\n",
    "error_df = pd.DataFrame(error_catching)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 455/455 [00:00<00:00, 898920.55it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_file(path_string, name_string):\n",
    "    with open(path_string.format(name_string), 'wb') as d:\n",
    "        d.write(myfile.content)\n",
    "\n",
    "save_path = '/Users/georgiarichards/Desktop/Python/PFDs opioids/All_PDFs8/{}.pdf'\n",
    "\n",
    "potential_names = ['ref', 'deceased_name', 'date_of_report']\n",
    "\n",
    "record_count = 0\n",
    "#This is the final scrape to actually get the URLs and change the name (when possible) to the refs\n",
    "for r_t, p_u in zip(tqdm(record_text), pdf_urls):\n",
    "    if not p_u:\n",
    "        #If there is no pdf at all, we skip it.\n",
    "        continue\n",
    "    else:\n",
    "        #All this does is gets the PDF and downloads it and names it after the reg\n",
    "        #It looks scary and complicated but all it is doing is varying the name in the case of multiple PDFs\n",
    "        #Or naming it for the deceased person if there is no Ref value\n",
    "        #If there is a pdf but no ref or deceased name, this will throw an error and we can adjust.\n",
    "        try:\n",
    "            counter = 0\n",
    "            if len(p_u) > 1:\n",
    "                for p in p_u:\n",
    "                    if counter == 0:\n",
    "                        myfile = get(p)\n",
    "                        named = False\n",
    "                        for x in potential_names:\n",
    "                            try:\n",
    "                                if r_t[x]:\n",
    "                                    save_file(save_path, r_t[x])\n",
    "                                    counter +=1\n",
    "                                    named = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        if not named:       \n",
    "                            save_file(save_path, 'check_record_{}'.format(record_count))\n",
    "                            counter +=1\n",
    "\n",
    "                    else:\n",
    "                        myfile = get(p)\n",
    "                        named = False\n",
    "                        for x in potential_names:\n",
    "                            try:\n",
    "                                if r_t[x]:\n",
    "                                    save_file(save_path, r_t[x] + '_{}'.format(counter))\n",
    "                                    counter +=1\n",
    "                                    named = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        if not named:\n",
    "                            save_file(save_path, 'check_record_{}_{}'.format(record_count, counter))\n",
    "                            counter +=1\n",
    "                                    \n",
    "            else:\n",
    "                myfile = get(p_u[0])\n",
    "                named = False\n",
    "                for x in potential_names:\n",
    "                    try:\n",
    "                        if r_t[x]:\n",
    "                            save_file(save_path, r_t[x])\n",
    "                            named = True\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                if not named:       \n",
    "                    save_file(save_path, 'check_record_{}'.format(record_count))\n",
    "            \n",
    "            record_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            import sys\n",
    "            if r_t['ref']:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record: {}'.format(r_t['ref'])).with_traceback(sys.exc_info()[2])\n",
    "            else:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record Number: {}'.format(record_count)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my final step that puts the text data (info on the deceased/case) into a csv file & adds the date it was pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "headers = ['date_of_report', 'date_of_reports', 'ref', 'deceased_name', 'deceased_names', 'coroner_name', 'coroner_area', 'category', 'this_report_is_being_sent_to', 'these_report_are_being_sent_to', 'url']\n",
    "\n",
    "with open('death_info_{}.csv'.format(date.today()), 'w', newline='', encoding='utf-8') as deaths_csv:\n",
    "    writer = csv.DictWriter(deaths_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for record in record_text:\n",
    "        if record == {}:\n",
    "            pass\n",
    "        else:\n",
    "            writer.writerow(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an addition few steps to check what differences there are from the June 2021 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'All_PDFs7'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pdfs7 \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m'\u001b[39;49m\u001b[39mAll_PDFs7\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m pdfs8 \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(\u001b[39m'\u001b[39m\u001b[39mAll_PDFs8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m new_not_old \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(pdfs8)\u001b[39m.\u001b[39mdifference(pdfs7)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'All_PDFs7'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pdfs7 = os.listdir('All_PDFs7')\n",
    "pdfs8 = os.listdir('All_PDFs8')\n",
    "\n",
    "new_not_old = set(pdfs8).difference(pdfs7)\n",
    "\n",
    "new_not_old_list = list(new_not_old)\n",
    "new_not_old_list.sort()\n",
    "new_not_old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_not_old_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'death_info_2022-02-23.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feb22 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdeath_info_2022-02-23.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m jun21 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdeath_info_2021-06-28.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/preventable/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/preventable/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/preventable/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/preventable/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/preventable/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'death_info_2022-02-23.csv'"
     ]
    }
   ],
   "source": [
    "feb22 = pd.read_csv('death_info_2022-02-23.csv')\n",
    "jun21 = pd.read_csv('death_info_2021-06-28.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(jun21.columns)\n",
    "merged = feb22.merge(jun21, on=cols, how='left', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only = merged[merged['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only.to_csv(r'death_info_newfeb22.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing for website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names = pd.read_csv('death_info_2022-02-23.csv')\n",
    "feb_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names['deceased_name'] = feb_names['deceased_name'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feb_names['deceased_name'] = feb_names['deceased_name'].apply(lambda x: ''.join(i[0] for i in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names['deceased_name'] = feb_names['deceased_name'].str.replace('\\W', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names.to_csv('death_info_2022-02-23_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
