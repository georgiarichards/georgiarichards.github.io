{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Judiciary Website to collect PFDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandercolby/opt/miniconda3/envs/preventable/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing libraries & setting up the Notebook  \n",
    "from requests import get\n",
    "from requests import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "from time import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "    \n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    " \n",
    "def get_url(url):\n",
    "    response = get(url, verify = False)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def retries(record_url, tries=3):\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            soup = get_url(record_url)\n",
    "            return soup\n",
    "        except (ConnectionError, SSLError):\n",
    "            if i < tries - 1:\n",
    "                sleep(2)\n",
    "                continue\n",
    "            else:\n",
    "                return 'Con error'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper starts here - run Sun 14 May 2023 9.15 pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:33<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4420 URLs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_url = 'https://www.judiciary.uk/prevention-of-future-death-reports/page/{}/'\n",
    "# update the below number based on the number of 'pages' on the Judiciary website \n",
    "page_count = 442\n",
    "\n",
    "with requests.Session() as session:\n",
    "    record_urls = [ ]\n",
    "    for page in tqdm(range(1, page_count + 1)):\n",
    "        url = base_url.format(page)\n",
    "        try:\n",
    "            response = session.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            record_urls += [a['href'] for a in soup.find_all('a', {'class': 'card__link'})]\n",
    "        except (requests.exceptions.RequestException, ValueError, AttributeError) as e:\n",
    "            print(f\"Failed to process page {page}: {e}\")\n",
    "    print(f\"Collected {len(record_urls)} URLs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4420"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we check how many records/cases/urls were found on the Judiciary website\n",
    "len(record_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.judiciary.uk/prevention-of-future-death-reports/stephen-richardson-prevention-of-future-deaths-report/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.judiciary.uk/prevention-of-future-death-reports/rachael-dallison/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record_urls[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the second loop that will go through the list of URLs from above to visit each individual record to pull out and store the text data into the a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_details(e_dict, record_count, record_url, details):\n",
    "    e_dict['index'] = record_count\n",
    "    e_dict['url'] = record_url\n",
    "    e_dict['reason'] = details\n",
    "    return e_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 660/4420 [01:43<09:58,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something we haven't accounted for has happened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 760/4420 [01:57<08:45,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something we haven't accounted for has happened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 795/4420 [02:03<09:03,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something we haven't accounted for has happened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 991/4420 [02:38<09:12,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something we haven't accounted for has happened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1032/4420 [02:44<06:59,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something we haven't accounted for has happened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 4393/4420 [11:16<00:03,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/railwayrelateddeaths/ produced no data\n",
      "https://www.judiciary.uk/prevention-of-future-death-reports/service-personnel-deaths/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 4395/4420 [11:16<00:04,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/product/ produced no data\n",
      "https://www.judiciary.uk/prevention-of-future-death-reports/policerelateddeaths/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 4396/4420 [11:16<00:03,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/carehomehealth/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4398/4420 [11:17<00:04,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/statecustodydeath/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 4400/4420 [11:17<00:03,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.judiciary.uk/prevention-of-future-death-reports/hospitaldeath/ produced no data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4420/4420 [11:22<00:00,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "reg_exp = re.compile(r\"’s\\s|s\\s|'s\\s\")\n",
    "text_cats = ['Date of report', 'Ref', 'Deceased name', 'Coroner name', 'Coroner Area', 'Category', \"This report is being sent to\"]\n",
    "#First, I create two lists, one for the PDFs and one for the text data\n",
    "record_text = []\n",
    "pdf_urls = []\n",
    "ref_list = []\n",
    "#I want to loop through each URL & pull out the death information and pdf link for downloading\n",
    "error_catching = []\n",
    "record_count = 0\n",
    "record_progress = tqdm(record_urls)\n",
    "\n",
    "for record_url in record_progress:\n",
    "    try:\n",
    "        error_dict = {}\n",
    "        #Calling the retries function\n",
    "        soup = retries(record_url, tries=5)\n",
    "        \n",
    "        if soup == 'Con error':\n",
    "            print(f\"{record_url} could not connect\")\n",
    "            error_catching.append(error_details(error_dict, record_count, record_url, 'Connection Error'))\n",
    "            record_count +=1\n",
    "            continue\n",
    "\n",
    "        #This gets all the text fields from the website to work with\n",
    "        death_info = soup.select_one('article.single__post.pfd.type-pfd').select('div > p')\n",
    "        \n",
    "        if not death_info:\n",
    "            print(f\"{record_url} produced no data\")\n",
    "            error_catching.append(error_details(error_dict, record_count, record_url, 'No Text Loaded'))\n",
    "            record_count +=1\n",
    "            continue\n",
    "            \n",
    "        #Our dictionary that will hold all of the text information that we will eventually append to \"record_text\"\n",
    "        blankdict = {}\n",
    "        \n",
    "        #This is to handle 1 annoying record with messed up html tags\n",
    "        if record_url == 'https://www.judiciary.uk/publications/roadsafety/':\n",
    "            strong = death_info[0].find_all('strong')\n",
    "            heads = ['date_of_report', 'ref', 'deceased_name', 'coroner_name', 'coroner_area', 'category']\n",
    "            for st, h in zip(strong,heads):\n",
    "                blankdict[h] = st.next_sibling.replace(':','').replace('Ref','').strip()\n",
    "        #And another record with wonky html\n",
    "        elif record_url == 'https://www.judiciary.uk/publications/helen-sheath/':\n",
    "            brs = death_info[0].text.split('\\n')\n",
    "            vals = []\n",
    "            for b in brs:\n",
    "                vals.append(b.split(':'))\n",
    "            for v in vals:\n",
    "                if v[0] == \"Coroners name\":\n",
    "                    alt = \"coroner_name\"\n",
    "                    blankdict[alt] = v[1].strip().replace('\\n','')\n",
    "                elif v[0] == \"Coroners Area\":\n",
    "                    alt = \"coroner_area\"\n",
    "                    blankdict[alt] = v[1].strip().replace('\\n','')\n",
    "                else:\n",
    "                    blankdict[v[0].strip().replace(' ','_').lower()] = v[1].strip().replace('\\n','')\n",
    "        else:        \n",
    "            #looping through all of the text categories for handling\n",
    "            for p in death_info:\n",
    "                #This checks for blank fields and if there is nothing, it skips it\n",
    "                if p.text.strip() == '':\n",
    "                    pass\n",
    "                #This checks for our \"Normal\" case in which a colon exists and the category is one of the ones we \n",
    "                #pre-specified above in the \"text_cats\" list\n",
    "                #We also need to account here for one strange record for \"Rebecca Evans\" which has a weird text error\n",
    "                #That we manually correct for\n",
    "                elif ':' in p.text and p.text.split(':')[0] in text_cats and not 'Rebecca-EvansR.pdf' in p.text:\n",
    "                    #Simply assigning the key and value from strings on either side of the colon, making everything \n",
    "                    #lower case and replacing spaces with underscores and also removing any stray semi-colons\n",
    "                    text_list = p.text.split(':')\n",
    "                    blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','').replace('\\xa0','')\n",
    "\n",
    "                elif 'Rebecca-EvansR.pdf' in p.text:\n",
    "                    #This deals with that singular odd record that currently exists as of 8 Nov 2019\n",
    "                    blankdict['category'] = p.text.split(':')[1].strip().replace('\\n','')\n",
    "                    \n",
    "                elif ':' not in p.text:\n",
    "                    #If the string doesn't have a colon, we can't split on it so have to get it into dictionary format\n",
    "                    #Using an alternate method that counts the length of the thing\n",
    "                    if any(x in p.text for x in text_cats):\n",
    "                        t = [x for x in text_cats if x in p.text][0]\n",
    "                        l = len(t)\n",
    "                        blankdict[t.replace(' ','_').lower()] = p.text[l+1:].replace('\\n','').replace('\\xa0','')\n",
    "                    elif 'Coroners Area' in p.text:\n",
    "                        blankdict['coroner_area'] = p.text[13:].strip().replace('\\n','').replace('\\xa0','')\n",
    "                    else:\n",
    "                        print(\"Something we haven't accounted for has happened\")\n",
    "\n",
    "                elif p.text.strip().count(\":\") == 2:\n",
    "                    #This corrects for one odd record in which there are 2 colons but should generalize to fix it for\n",
    "                    #any time this could happen, so long as it happens in the same way\n",
    "                    text_list = p.text.split(':')\n",
    "                    new_string = text_list[0] + text_list[1]\n",
    "                    new_name = re.sub(reg_exp, ' ', new_string).strip()\n",
    "                    blankdict[new_name.replace(' ','_').lower()] = text_list[2].strip().replace('\\n','').replace('\\xa0','')\n",
    "\n",
    "                elif ':' in p.text and p.text.split(':')[0] not in text_cats:\n",
    "                    #Some field names are in the form of \"name_of_decesased\" or \"name_of_coroner\" or are plural/\n",
    "                    #possessive so this smashes those into our preferred naming formats\n",
    "                    if 'Name of' in p.text:\n",
    "                        all_text = p.text.split(':')\n",
    "                        key_name = all_text[0].split(' ')\n",
    "                        blankdict[key_name[2].strip() + '_name'] = all_text[-1].strip()\n",
    "                    else:    \n",
    "                        new_name = re.sub(reg_exp, ' ', p.text)\n",
    "                        text_list = new_name.split(':')\n",
    "                        blankdict[text_list[0].strip().replace(' ','_').lower()] = text_list[1].strip().replace('\\n','').replace('\\xa0','')\n",
    "        blankdict['url'] = record_url\n",
    "        \n",
    "        #A small little check for duplicated ref names\n",
    "        try:\n",
    "            if not blankdict['ref']:\n",
    "                pass\n",
    "            elif blankdict['ref'] in ref_list:\n",
    "                blankdict['ref'] = blankdict['ref'] + 'A'\n",
    "            ref_list.append(blankdict['ref'])\n",
    "        except KeyError:\n",
    "            blankdict['ref'] = ''\n",
    "            \n",
    "        #This appends the final dict to the list\n",
    "        record_text.append(blankdict)\n",
    "        \n",
    "        #this is a seperate process to get the PDF URLs (no matter how many there are) and adds them to their own list   \n",
    "        urls = soup.select('li.related-content__item')\n",
    "        pdf_list = []\n",
    "        for url in urls:\n",
    "            pdf_list.append(url.findNext('a').get('href'))\n",
    "        pdf_urls.append(pdf_list)\n",
    "        \n",
    "        record_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        import sys\n",
    "        error_desc = f\"{str(e)} occurred for {record_url} when trying to work with {p}\"\n",
    "        error_n = f\"{record_count}: {str(e)}\"\n",
    "        short_desc = error_n[:50] + '...' if len(error_n) > 50 else error_n\n",
    "        record_progress.set_description(short_desc)\n",
    "        # print(error_desc)\n",
    "        error_catching.append(error_details(error_dict, record_count, record_url, error_desc))\n",
    "        \n",
    "        #Saving this in case we don't like the error catching.\n",
    "        #import sys\n",
    "        #raise type(e)(str(e) + '\\n' + 'Error for Record: {}, Field: {}'.format(record_url, p)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the third loop to save the PDFs using the deceased Ref as the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>url</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4391</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>No Text Loaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4392</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>No Text Loaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4393</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>No Text Loaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4394</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>No Text Loaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4395</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>No Text Loaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4396</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>No Text Loaded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4398</td>\n",
       "      <td>https://www.judiciary.uk/prevention-of-future-...</td>\n",
       "      <td>No Text Loaded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                url          reason\n",
       "0   4391  https://www.judiciary.uk/prevention-of-future-...  No Text Loaded\n",
       "1   4392  https://www.judiciary.uk/prevention-of-future-...  No Text Loaded\n",
       "2   4393  https://www.judiciary.uk/prevention-of-future-...  No Text Loaded\n",
       "3   4394  https://www.judiciary.uk/prevention-of-future-...  No Text Loaded\n",
       "4   4395  https://www.judiciary.uk/prevention-of-future-...  No Text Loaded\n",
       "5   4396  https://www.judiciary.uk/prevention-of-future-...  No Text Loaded\n",
       "6   4398  https://www.judiciary.uk/prevention-of-future-...  No Text Loaded"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Any errors should print out above, but you can also check the error_catching dict\n",
    "#Here we just turn it into a dataframe quickly to easily view\n",
    "\n",
    "error_df = pd.DataFrame(error_catching)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Desktop/Python/PFDs opioids/All_PDFs8/2023-0209.pdf'\nError for Record: 2023-0209",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 66\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m r_t[x]:\n\u001b[0;32m---> 66\u001b[0m     save_file(save_path, r_t[x])\n\u001b[1;32m     67\u001b[0m     named \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(path_string, name_string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_file\u001b[39m(path_string, name_string):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path_string\u001b[39m.\u001b[39;49mformat(name_string), \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m d:\n\u001b[1;32m      3\u001b[0m         d\u001b[39m.\u001b[39mwrite(myfile\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/preventable/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Desktop/Python/PFDs opioids/All_PDFs8/2023-0209.pdf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mif\u001b[39;00m r_t[\u001b[39m'\u001b[39m\u001b[39mref\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\u001b[39mstr\u001b[39m(e) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mError for Record: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(r_t[\u001b[39m'\u001b[39m\u001b[39mref\u001b[39m\u001b[39m'\u001b[39m]))\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n\u001b[1;32m     82\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mtype\u001b[39m(e)(\u001b[39mstr\u001b[39m(e) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mError for Record Number: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(record_count))\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[11], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m r_t[x]:\n\u001b[0;32m---> 66\u001b[0m         save_file(save_path, r_t[x])\n\u001b[1;32m     67\u001b[0m         named \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     68\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(path_string, name_string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_file\u001b[39m(path_string, name_string):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path_string\u001b[39m.\u001b[39;49mformat(name_string), \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m d:\n\u001b[1;32m      3\u001b[0m         d\u001b[39m.\u001b[39mwrite(myfile\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/preventable/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Desktop/Python/PFDs opioids/All_PDFs8/2023-0209.pdf'\nError for Record: 2023-0209"
     ]
    }
   ],
   "source": [
    "def save_file(path_string, name_string):\n",
    "    with open(path_string.format(name_string), 'wb') as d:\n",
    "        d.write(myfile.content)\n",
    "\n",
    "save_path = '~/Desktop/Python/PFDs opioids/All_PDFs8/{}.pdf'\n",
    "\n",
    "potential_names = ['ref', 'deceased_name', 'date_of_report']\n",
    "\n",
    "record_count = 0\n",
    "#This is the final scrape to actually get the URLs and change the name (when possible) to the refs\n",
    "for r_t, p_u in zip(tqdm(record_text), pdf_urls):\n",
    "    if not p_u:\n",
    "        #If there is no pdf at all, we skip it.\n",
    "        continue\n",
    "    else:\n",
    "        #All this does is gets the PDF and downloads it and names it after the reg\n",
    "        #It looks scary and complicated but all it is doing is varying the name in the case of multiple PDFs\n",
    "        #Or naming it for the deceased person if there is no Ref value\n",
    "        #If there is a pdf but no ref or deceased name, this will throw an error and we can adjust.\n",
    "        try:\n",
    "            counter = 0\n",
    "            if len(p_u) > 1:\n",
    "                for p in p_u:\n",
    "                    if counter == 0:\n",
    "                        myfile = get(p)\n",
    "                        named = False\n",
    "                        for x in potential_names:\n",
    "                            try:\n",
    "                                if r_t[x]:\n",
    "                                    save_file(save_path, r_t[x])\n",
    "                                    counter +=1\n",
    "                                    named = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        if not named:       \n",
    "                            save_file(save_path, 'check_record_{}'.format(record_count))\n",
    "                            counter +=1\n",
    "\n",
    "                    else:\n",
    "                        myfile = get(p)\n",
    "                        named = False\n",
    "                        for x in potential_names:\n",
    "                            try:\n",
    "                                if r_t[x]:\n",
    "                                    save_file(save_path, r_t[x] + '_{}'.format(counter))\n",
    "                                    counter +=1\n",
    "                                    named = True\n",
    "                                    break\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except KeyError:\n",
    "                                continue\n",
    "                        if not named:\n",
    "                            save_file(save_path, 'check_record_{}_{}'.format(record_count, counter))\n",
    "                            counter +=1\n",
    "                                    \n",
    "            else:\n",
    "                myfile = get(p_u[0])\n",
    "                named = False\n",
    "                for x in potential_names:\n",
    "                    try:\n",
    "                        if r_t[x]:\n",
    "                            save_file(save_path, r_t[x])\n",
    "                            named = True\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                if not named:       \n",
    "                    save_file(save_path, 'check_record_{}'.format(record_count))\n",
    "            \n",
    "            record_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            import sys\n",
    "            if r_t['ref']:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record: {}'.format(r_t['ref'])).with_traceback(sys.exc_info()[2])\n",
    "            else:\n",
    "                raise type(e)(str(e) + '\\n' + 'Error for Record Number: {}'.format(record_count)).with_traceback(sys.exc_info()[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step consolidates headers that match semantically but are mispelled or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_map = {\n",
    "  \"date_of_report\": \"date_of_report\",\n",
    "  \"date_of_reports\": \"date_of_report\",\n",
    "\n",
    "  \"ref\": \"ref\",\n",
    "\n",
    "  \"deceased_name\": \"deceased_name\",\n",
    "  \"deceased_names\": \"deceased_name\",\n",
    "\n",
    "  \"coroner_name\": \"coroner_name\",\n",
    "  \"coroner_names\": \"coroner_name\",\n",
    "\n",
    "  \"coroner_area\": \"coroner_area\",\n",
    "  \"coroner_areas\": \"coroner_area\",\n",
    "\n",
    "  \"category\": \"category\",\n",
    "\n",
    "  \"this_report_is_being_sent_to\": \"this_report_is_being_sent_to\",\n",
    "  \"these_report_are_being_sent_to\": \"this_report_is_being_sent_to\",\n",
    "  \"these_report_have_been_sent_to\": \"this_report_is_being_sent_to\",\n",
    "  \"thi_i_being_sent_to\": \"this_report_is_being_sent_to\",\n",
    "  \"thi_ha_been_sent_to\": \"this_report_is_being_sent_to\",\n",
    "\n",
    "  \"url\": \"url\"\n",
    "}\n",
    "\n",
    "record_text = [{header_map[k]: v for k, v in record.items()} for record in record_text]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my final step that puts the text data (info on the deceased/case) into a csv file & adds the date it was pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "headers = ['date_of_report', 'date_of_reports', 'ref', 'deceased_name', 'deceased_names', 'coroner_name', 'coroner_area', 'category', 'this_report_is_being_sent_to', 'these_report_are_being_sent_to', 'url']\n",
    "\n",
    "with open('death_info_{}.csv'.format(date.today()), 'w', newline='', encoding='utf-8') as deaths_csv:\n",
    "    writer = csv.DictWriter(deaths_csv, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    for record in record_text:\n",
    "        if record == {}:\n",
    "            pass\n",
    "        else:\n",
    "            writer.writerow(record)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an addition few steps to check what differences there are from the June 2021 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pdfs7 = os.listdir('All_PDFs7')\n",
    "pdfs8 = os.listdir('All_PDFs8')\n",
    "\n",
    "new_not_old = set(pdfs8).difference(pdfs7)\n",
    "\n",
    "new_not_old_list = list(new_not_old)\n",
    "new_not_old_list.sort()\n",
    "new_not_old_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_not_old_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb22 = pd.read_csv('death_info_2022-02-23.csv')\n",
    "jun21 = pd.read_csv('death_info_2021-06-28.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(jun21.columns)\n",
    "merged = feb22.merge(jun21, on=cols, how='left', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only = merged[merged['_merge'] == 'left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_only.to_csv(r'death_info_newfeb22.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing for website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names = pd.read_csv('death_info_2022-02-23.csv')\n",
    "feb_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names['deceased_name'] = feb_names['deceased_name'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feb_names['deceased_name'] = feb_names['deceased_name'].apply(lambda x: ''.join(i[0] for i in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names['deceased_name'] = feb_names['deceased_name'].str.replace('\\W', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_names.to_csv('death_info_2022-02-23_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
